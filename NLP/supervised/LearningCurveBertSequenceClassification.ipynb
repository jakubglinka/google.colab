{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningCurveBertSequenceClassification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakubglinka/google.colab/blob/master/NLP/supervised/LearningCurveBertSequenceClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4AyWOwVo_gJ",
        "colab_type": "text"
      },
      "source": [
        "# Text Classification using BERT\n",
        "\n",
        " - using pre-trained bert model\n",
        " - learning curve"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwrWFxeDr3-a",
        "colab_type": "text"
      },
      "source": [
        "## Configure environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDgjbvfoos5e",
        "colab_type": "code",
        "outputId": "c60f5113-93ff-4185-ec20-43930c0b55c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKMQ2ytGpSu1",
        "colab_type": "code",
        "outputId": "3ef09347-7e39-47b6-ccc9-f49898e9fb63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "Num GPUs Available:  0\n",
            "2.1.0-rc1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJJJylM2OBCa",
        "colab_type": "code",
        "outputId": "8eed0e91-224c-4f97-bea9-8d04a6bb1765",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        }
      },
      "source": [
        "try:\n",
        "  cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "  tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "  tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "except ValueError as error:\n",
        "  print(error)\n",
        "  print(\"No TPU available. Switching to single device strategy.\")\n",
        "  strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.111.174.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.111.174.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.111.174.138:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.111.174.138:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1GG_p1hrACo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "35bfd6d0-e6ba-4ec4-8844-971d2e987485"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install git+https://jbglin:botrx56jtlp6p2cbsthvt3bkslgeo3pzc5c7iuu4irxscjmmc6xa@dev.azure.com/eyDataScienceTeam/_git/nlp-ey-assets@develop"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.3.0)\n",
            "Requirement already satisfied: requests in /tensorflow-2.1.0/python3.6 (from transformers) (2.22.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.85)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.9)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.38)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /tensorflow-2.1.0/python3.6 (from transformers) (1.17.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests->transformers) (1.25.7)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.40 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.40)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from sacremoses->transformers) (1.13.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.40->boto3->transformers) (2.6.1)\n",
            "Collecting git+https://jbglin:****@dev.azure.com/eyDataScienceTeam/_git/nlp-ey-assets@develop\n",
            "  Cloning https://jbglin:****@dev.azure.com/eyDataScienceTeam/_git/nlp-ey-assets (to revision develop) to /tmp/pip-req-build-doheuzft\n",
            "  Running command git clone -q 'https://jbglin:****@dev.azure.com/eyDataScienceTeam/_git/nlp-ey-assets' /tmp/pip-req-build-doheuzft\n",
            "  Running command git checkout -b develop --track origin/develop\n",
            "  Switched to a new branch 'develop'\n",
            "  Branch 'develop' set up to track remote branch 'develop' from 'origin'.\n",
            "Requirement already satisfied (use --upgrade to upgrade): nlp-ey-assets==0.1.dev0 from git+https://jbglin:****@dev.azure.com/eyDataScienceTeam/_git/nlp-ey-assets@develop in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: absl-py>=0.7.1<1.0.0 in /tensorflow-2.1.0/python3.6 (from nlp-ey-assets==0.1.dev0) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=4.32.2<5.0.0 in /usr/local/lib/python3.6/dist-packages (from nlp-ey-assets==0.1.dev0) (4.41.1)\n",
            "Requirement already satisfied: wget>=3.2<4.0 in /usr/local/lib/python3.6/dist-packages (from nlp-ey-assets==0.1.dev0) (3.2)\n",
            "Requirement already satisfied: tensorflow>=2.0.0 in /tensorflow-2.1.0/python3.6 (from nlp-ey-assets==0.1.dev0) (2.1.0rc1)\n",
            "Requirement already satisfied: six in /tensorflow-2.1.0/python3.6 (from absl-py>=0.7.1<1.0.0->nlp-ey-assets==0.1.dev0) (1.13.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.17.4)\n",
            "Requirement already satisfied: astor>=0.6.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.11.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.25.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.33.6)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.1.8)\n",
            "Requirement already satisfied: gast==0.2.2 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-2.1.0/python3.6 (from tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /tensorflow-2.1.0/python3.6 (from protobuf>=3.8.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (42.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.1.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.9.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.16.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2.22.0)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from keras-applications>=1.0.8->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2.10.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /tensorflow-2.1.0/python3.6 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.2.7)\n",
            "Requirement already satisfied: cachetools<3.2,>=2.0.0 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.1.1)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (4.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (1.25.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (2019.11.28)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /tensorflow-2.1.0/python3.6 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /tensorflow-2.1.0/python3.6 (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow>=2.0.0->nlp-ey-assets==0.1.dev0) (0.4.8)\n",
            "Building wheels for collected packages: nlp-ey-assets\n",
            "  Building wheel for nlp-ey-assets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nlp-ey-assets: filename=nlp_ey_assets-0.1.dev0-cp36-none-any.whl size=87765 sha256=fae53bf390a92c06334585bd6e3a863eb8a78d01402add053b3809035217e61e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-69gpx5oz/wheels/82/e1/b0/516034573f2c111d58a616e4e64b46cd8d80f52f98f07a64bb\n",
            "Successfully built nlp-ey-assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etw763McAqXP",
        "colab_type": "text"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5jwC3wqa8c-",
        "colab_type": "text"
      },
      "source": [
        "### PolEmo data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lFdOILpAw91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pathlib\n",
        "import re\n",
        "import tqdm\n",
        "POLEMO_PATH = \"./drive/My Drive/sentiment/\"\n",
        "from typing import List\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe60MzVMCB44",
        "colab_type": "code",
        "outputId": "4189e7fc-ba9d-4eea-9c37-f7b293a30c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "# read PolEmo data:\n",
        "def read_polemo_data(path) -> pd.DataFrame:\n",
        "  res = []\n",
        "  with path.open(\"r\") as f:\n",
        "    for line in f:\n",
        "      rec = line.strip(\"\\n\").split(\"__label__\")\n",
        "      rec[0] = rec[0].strip()\n",
        "      res.append(rec)\n",
        "\n",
        "  return pd.DataFrame(res, columns=[\"text\", \"label\"])\n",
        "\n",
        "df_train = read_polemo_data(pathlib.Path(POLEMO_PATH) / \"all.sentence.train.txt\")\n",
        "print(f\"Read {df_train.shape[0]} train examples.\")\n",
        "\n",
        "df_dev = read_polemo_data(pathlib.Path(POLEMO_PATH) / \"all.sentence.dev.txt\")\n",
        "print(f\"Read {df_dev.shape[0]} dev examples.\")\n",
        "\n",
        "df_test = read_polemo_data(pathlib.Path(POLEMO_PATH) / \"all.sentence.test.txt\")\n",
        "print(f\"Read {df_test.shape[0]} test examples.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 45974 train examples.\n",
            "Read 5747 dev examples.\n",
            "Read 5745 test examples.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Balu-CpqND",
        "colab_type": "code",
        "outputId": "32495453-0e47-4436-e281-80472b995831",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "df_train.iloc[5745, :]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     Niestety mam podobne odczucia jak poprzedniczka .\n",
              "label                                            z_minus_m\n",
              "Name: 5745, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCcsjatvwvOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "enc = LabelEncoder().fit(df_train.label.values)\n",
        "\n",
        "# add encoded labels:\n",
        "df_train[\"label_enc\"] = enc.transform(df_train[\"label\"])\n",
        "df_dev[\"label_enc\"] = enc.transform(df_dev[\"label\"])\n",
        "df_test[\"label_enc\"] = enc.transform(df_test[\"label\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fha1df-nFYvM",
        "colab_type": "text"
      },
      "source": [
        "## Pre-Trained BERT\n",
        " - bert-base-multilingual-cased"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2cKhzk2QZMo",
        "colab_type": "text"
      },
      "source": [
        "### Prepare BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKQmSQQHkvGA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "42e1a55d-448c-4677-e5aa-f73a805cfafc"
      },
      "source": [
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "\n",
        "# create new instance of BertTokenizer\n",
        "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
        "print(bert_tokenizer.tokenize(\"Ala ma kota!\"))\n",
        "\n",
        "# encode plus:\n",
        "print(bert_tokenizer.encode_plus(\"Ala ma kota!\", max_length=10, pad_to_max_length=True))\n",
        "\n",
        "# check for number of tokens distribution\n",
        "df_train[\"n_tokens\"] = df_train.text.apply(lambda x: len(bert_tokenizer.tokenize(x)))\n",
        "np.quantile(df_train.n_tokens, q=[.9, .99, .999])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Ala', 'ma', 'kota', '!']\n",
            "{'input_ids': [101, 56500, 10824, 16469, 106, 102, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 59.   , 122.   , 243.027])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNOwG5D0yYad",
        "colab_type": "text"
      },
      "source": [
        "### TFRecords"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8UUgNzMmWS6",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare single Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvZErYyfZz_h",
        "colab_type": "code",
        "outputId": "a6bdb5e0-773f-466d-9212-2ce34afa82a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "bert_tokenizer.tokenize(\"Ala ma kota!\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ala', 'ma', 'kota', '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQZiJc5lFqBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import Dict\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "# prepare data:\n",
        "def _prepare_single_example(x: str, max_seq_len: int = None, label: int = 0) -> Dict:\n",
        "  \n",
        "  inputs = bert_tokenizer.encode_plus(x, max_length=max_seq_len, pad_to_max_length=False, return_tensors=\"tf\")\n",
        "  inputs[\"attention_mask\"] = tf.squeeze(inputs[\"attention_mask\"], axis=0)\n",
        "  inputs[\"input_ids\"] = tf.squeeze(inputs[\"input_ids\"], axis=0)\n",
        "  inputs[\"token_type_ids\"] = tf.squeeze(inputs[\"token_type_ids\"], axis=0)\n",
        "  \n",
        "  inputs[\"label\"] = tf.convert_to_tensor(label)\n",
        "  inputs[\"raw_text\"] = tf.convert_to_tensor(x)\n",
        "\n",
        "  return inputs\n",
        "\n",
        "# serialize:\n",
        "def _serialize_example(example):\n",
        "  \n",
        "  # attention mask\n",
        "  example[\"attention_mask\"] = tf.io.serialize_tensor(example[\"attention_mask\"]).numpy()\n",
        "  example[\"attention_mask\"] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[example[\"attention_mask\"]]))\n",
        "  \n",
        "  # input ids\n",
        "  example[\"input_ids\"] = tf.io.serialize_tensor(example[\"input_ids\"]).numpy()\n",
        "  example[\"input_ids\"] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[example[\"input_ids\"]]))\n",
        "\n",
        "  # token type ids\n",
        "  example[\"token_type_ids\"] = tf.io.serialize_tensor(example[\"token_type_ids\"]).numpy()\n",
        "  example[\"token_type_ids\"] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[example[\"token_type_ids\"]]))\n",
        "\n",
        "  example[\"raw_text\"] = tf.train.Feature(bytes_list = tf.train.BytesList(value=[example[\"raw_text\"].numpy()]))\n",
        "  example[\"label\"] = tf.train.Feature(int64_list = tf.train.Int64List(value=[example[\"label\"].numpy()]))\n",
        "\n",
        "  ex = tf.train.Example(features=tf.train.Features(feature=example))\n",
        "  return ex.SerializeToString()  \n",
        "\n",
        "# example = _prepare_single_example(\"Ala ma kota!\", 64, 1)\n",
        "# example\n",
        "# tf.train.Example.FromString(_serialize_example(example))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FtT5xQBu7tg",
        "colab_type": "text"
      },
      "source": [
        "#### Prepare TFRecords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hCk4dGhesTQn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "outputId": "47da13ef-65b7-4c54-c5c4-30edd22403d6"
      },
      "source": [
        "!rm ./train.TFRecord ./dev.TFRecord ./test.TFRecord"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: cannot remove './train.TFRecord': No such file or directory\n",
            "rm: cannot remove './dev.TFRecord': No such file or directory\n",
            "rm: cannot remove './test.TFRecord': No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A6K3OcrlmemW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "45d05613-c260-4775-9f1f-28f0f76bee2e"
      },
      "source": [
        "np.random.seed(1234)\n",
        "df_train[\"chunk\"] = np.random.choice(range(int(df_train.shape[0]/1000)), size=df_train.shape[0])\n",
        "df_train[\"chunk\"].describe()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    45974.000000\n",
              "mean        21.921173\n",
              "std         12.999854\n",
              "min          0.000000\n",
              "25%         11.000000\n",
              "50%         22.000000\n",
              "75%         33.000000\n",
              "max         44.000000\n",
              "Name: chunk, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsff3xpKKzVa",
        "colab_type": "code",
        "outputId": "7175a322-8984-420d-c450-865958b3f912",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        }
      },
      "source": [
        "# Write the `tf.Example` observations to the file.\n",
        "\n",
        "for chunk in tqdm.tqdm(range(int(df_train.shape[0]/1000))):\n",
        "  with tf.io.TFRecordWriter(f\"./train_chunk_{chunk}.TFRecord\") as writer:\n",
        "    rows = df_train[df_train.chunk==chunk].iterrows()\n",
        "    for __, row in rows:\n",
        "      example = _prepare_single_example(row.text, None, row.label_enc)\n",
        "      writer.write(_serialize_example(example))\n",
        "\n",
        "with tf.io.TFRecordWriter(\"./dev.TFRecord\") as writer:\n",
        "  for __, row in tqdm.tqdm(df_dev.iterrows()):\n",
        "    example = _prepare_single_example(row.text, None, row.label_enc)\n",
        "    writer.write(_serialize_example(example))\n",
        "\n",
        "with tf.io.TFRecordWriter(\"./test.TFRecord\") as writer:\n",
        "  for __, row in tqdm.tqdm(df_test.iterrows()):\n",
        "    example = _prepare_single_example(row.text, None, row.label_enc)\n",
        "    writer.write(_serialize_example(example))\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 45/45 [06:13<00:00,  8.31s/it]\n",
            "5747it [00:54, 105.66it/s]\n",
            "5745it [00:52, 109.51it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2iTdKDIBIuc",
        "colab_type": "code",
        "outputId": "3e34e2ea-580e-4b2f-90f1-4711495c536d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "!ls -la -h | grep TF"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 3.8M Jan  7 20:23 dev.TFRecord\n",
            "-rw-r--r-- 1 root root 3.8M Jan  7 20:24 test.TFRecord\n",
            "-rw-r--r-- 1 root root 693K Jan  7 20:16 train_chunk_0.TFRecord\n",
            "-rw-r--r-- 1 root root 663K Jan  7 20:18 train_chunk_10.TFRecord\n",
            "-rw-r--r-- 1 root root 688K Jan  7 20:18 train_chunk_11.TFRecord\n",
            "-rw-r--r-- 1 root root 668K Jan  7 20:18 train_chunk_12.TFRecord\n",
            "-rw-r--r-- 1 root root 681K Jan  7 20:18 train_chunk_13.TFRecord\n",
            "-rw-r--r-- 1 root root 698K Jan  7 20:18 train_chunk_14.TFRecord\n",
            "-rw-r--r-- 1 root root 639K Jan  7 20:19 train_chunk_15.TFRecord\n",
            "-rw-r--r-- 1 root root 716K Jan  7 20:19 train_chunk_16.TFRecord\n",
            "-rw-r--r-- 1 root root 650K Jan  7 20:19 train_chunk_17.TFRecord\n",
            "-rw-r--r-- 1 root root 670K Jan  7 20:19 train_chunk_18.TFRecord\n",
            "-rw-r--r-- 1 root root 658K Jan  7 20:19 train_chunk_19.TFRecord\n",
            "-rw-r--r-- 1 root root 696K Jan  7 20:17 train_chunk_1.TFRecord\n",
            "-rw-r--r-- 1 root root 705K Jan  7 20:19 train_chunk_20.TFRecord\n",
            "-rw-r--r-- 1 root root 686K Jan  7 20:19 train_chunk_21.TFRecord\n",
            "-rw-r--r-- 1 root root 694K Jan  7 20:20 train_chunk_22.TFRecord\n",
            "-rw-r--r-- 1 root root 642K Jan  7 20:20 train_chunk_23.TFRecord\n",
            "-rw-r--r-- 1 root root 685K Jan  7 20:20 train_chunk_24.TFRecord\n",
            "-rw-r--r-- 1 root root 646K Jan  7 20:20 train_chunk_25.TFRecord\n",
            "-rw-r--r-- 1 root root 659K Jan  7 20:20 train_chunk_26.TFRecord\n",
            "-rw-r--r-- 1 root root 706K Jan  7 20:20 train_chunk_27.TFRecord\n",
            "-rw-r--r-- 1 root root 640K Jan  7 20:20 train_chunk_28.TFRecord\n",
            "-rw-r--r-- 1 root root 678K Jan  7 20:20 train_chunk_29.TFRecord\n",
            "-rw-r--r-- 1 root root 693K Jan  7 20:17 train_chunk_2.TFRecord\n",
            "-rw-r--r-- 1 root root 641K Jan  7 20:21 train_chunk_30.TFRecord\n",
            "-rw-r--r-- 1 root root 650K Jan  7 20:21 train_chunk_31.TFRecord\n",
            "-rw-r--r-- 1 root root 673K Jan  7 20:21 train_chunk_32.TFRecord\n",
            "-rw-r--r-- 1 root root 720K Jan  7 20:21 train_chunk_33.TFRecord\n",
            "-rw-r--r-- 1 root root 697K Jan  7 20:21 train_chunk_34.TFRecord\n",
            "-rw-r--r-- 1 root root 635K Jan  7 20:21 train_chunk_35.TFRecord\n",
            "-rw-r--r-- 1 root root 625K Jan  7 20:21 train_chunk_36.TFRecord\n",
            "-rw-r--r-- 1 root root 642K Jan  7 20:22 train_chunk_37.TFRecord\n",
            "-rw-r--r-- 1 root root 635K Jan  7 20:22 train_chunk_38.TFRecord\n",
            "-rw-r--r-- 1 root root 694K Jan  7 20:22 train_chunk_39.TFRecord\n",
            "-rw-r--r-- 1 root root 692K Jan  7 20:17 train_chunk_3.TFRecord\n",
            "-rw-r--r-- 1 root root 661K Jan  7 20:22 train_chunk_40.TFRecord\n",
            "-rw-r--r-- 1 root root 684K Jan  7 20:22 train_chunk_41.TFRecord\n",
            "-rw-r--r-- 1 root root 658K Jan  7 20:22 train_chunk_42.TFRecord\n",
            "-rw-r--r-- 1 root root 693K Jan  7 20:22 train_chunk_43.TFRecord\n",
            "-rw-r--r-- 1 root root 704K Jan  7 20:23 train_chunk_44.TFRecord\n",
            "-rw-r--r-- 1 root root 683K Jan  7 20:17 train_chunk_4.TFRecord\n",
            "-rw-r--r-- 1 root root 700K Jan  7 20:17 train_chunk_5.TFRecord\n",
            "-rw-r--r-- 1 root root 692K Jan  7 20:17 train_chunk_6.TFRecord\n",
            "-rw-r--r-- 1 root root 687K Jan  7 20:17 train_chunk_7.TFRecord\n",
            "-rw-r--r-- 1 root root 696K Jan  7 20:18 train_chunk_8.TFRecord\n",
            "-rw-r--r-- 1 root root 648K Jan  7 20:18 train_chunk_9.TFRecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKqTVqyVxR2u",
        "colab_type": "text"
      },
      "source": [
        "#### Copy to GC Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWvoy79zxXTk",
        "colab_type": "code",
        "outputId": "749a9caa-4897-4115-db2a-783fcda34283",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'southern-shard-211411'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOHKKI9XxXS9",
        "colab_type": "code",
        "outputId": "15f6d415-4998-4eb7-d108-feb620a7ad9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 889
        }
      },
      "source": [
        "# Upload the files to a given Google Cloud Storage bucket.\n",
        "!gsutil -m cp ./train*.TFRecord gs://tf_experiments_records/PolEmo/bert/\n",
        "!gsutil cp ./dev.TFRecord gs://tf_experiments_records/PolEmo/bert/dev.TFRecord\n",
        "!gsutil cp ./test.TFRecord gs://tf_experiments_records/PolEmo/bert/test.TFRecord"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying file://./train_chunk_0.TFRecord [Content-Type=application/octet-stream]...\n",
            "/ [0/45 files][    0.0 B/ 29.6 MiB]   0% Done                                   \rCopying file://./train_chunk_10.TFRecord [Content-Type=application/octet-stream]...\n",
            "/ [0/45 files][    0.0 B/ 29.6 MiB]   0% Done                                   \rCopying file://./train_chunk_18.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_15.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_16.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_17.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_14.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_13.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_11.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_12.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_19.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_1.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_20.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_21.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_22.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_23.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_24.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_25.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_26.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_27.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_28.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_29.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_2.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_30.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_31.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_32.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_33.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_34.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_35.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_36.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_37.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_38.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_39.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_3.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_40.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_41.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_42.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_43.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_44.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_4.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_5.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_6.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_7.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_8.TFRecord [Content-Type=application/octet-stream]...\n",
            "Copying file://./train_chunk_9.TFRecord [Content-Type=application/octet-stream]...\n",
            "| [45/45 files][ 29.6 MiB/ 29.6 MiB] 100% Done                                  \n",
            "Operation completed over 45 objects/29.6 MiB.                                    \n",
            "Copying file://./dev.TFRecord [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  3.8 MiB/  3.8 MiB]                                                \n",
            "Operation completed over 1 objects/3.8 MiB.                                      \n",
            "Copying file://./test.TFRecord [Content-Type=application/octet-stream]...\n",
            "/ [1 files][  3.7 MiB/  3.7 MiB]                                                \n",
            "Operation completed over 1 objects/3.7 MiB.                                      \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enJpe7ZBB8L8",
        "colab_type": "text"
      },
      "source": [
        "### Read from GC Bucket"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_6fyYLkybS0",
        "colab_type": "code",
        "outputId": "fecaffb0-5ec9-4056-c636-029ed7547e2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# https://cloud.google.com/resource-manager/docs/creating-managing-projects\n",
        "project_id = 'southern-shard-211411'\n",
        "!gcloud config set project {project_id}"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vldThzyv3iNy",
        "colab_type": "code",
        "outputId": "5c7f4b75-557d-445f-8b4d-8747762817c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        }
      },
      "source": [
        "# Set access for the TPU pod:\n",
        "!gsutil -m acl ch -u service-495559152420@cloud-tpu.iam.gserviceaccount.com:READER gs://tf_experiments_records/PolEmo/bert/*"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No changes to gs://tf_experiments_records/PolEmo/bert/dev.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/test.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_1.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_0.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_13.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_10.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_11.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_14.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_2.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_12.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_15.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_17.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_16.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_18.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_19.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_20.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_21.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_22.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_23.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_24.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_25.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_27.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_28.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_26.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_3.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_29.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_30.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_31.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_32.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_33.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_34.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_37.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_36.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_35.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_39.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_40.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_4.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_41.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_38.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_42.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_43.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_44.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_5.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_6.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_7.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_8.TFRecord\n",
            "No changes to gs://tf_experiments_records/PolEmo/bert/train_chunk_9.TFRecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hn3fKZtFC1MK",
        "colab_type": "code",
        "outputId": "9dd03c5f-f345-4ddb-8902-92c67dbc954f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "# Create a dictionary describing the features.\n",
        "_feature_description = {\n",
        "    'attention_mask': tf.io.FixedLenFeature([], tf.string),\n",
        "    'input_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "    'token_type_ids': tf.io.FixedLenFeature([], tf.string),\n",
        "    'label': tf.io.FixedLenFeature([], tf.int64)\n",
        "}\n",
        "\n",
        "def _parse_data(example_proto, max_seq_len: int = 128):\n",
        "  # Parse the input tf.Example proto using the dictionary above.\n",
        "  rec = tf.io.parse_single_example(example_proto, _feature_description)\n",
        "  \n",
        "  # attention_mask:\n",
        "  rec[\"attention_mask\"] = tf.io.parse_tensor(rec[\"attention_mask\"], out_type=tf.int32)\n",
        "  rec[\"attention_mask\"] = rec[\"attention_mask\"][:max_seq_len]\n",
        "  n_tokens = tf.shape(rec[\"attention_mask\"])[0]\n",
        "  padding = max_seq_len - n_tokens\n",
        "  rec[\"attention_mask\"] = tf.pad(rec[\"attention_mask\"], paddings=[[0, padding]])\n",
        "\n",
        "  # input_ids:\n",
        "  rec[\"input_ids\"] = tf.io.parse_tensor(rec[\"input_ids\"], out_type=tf.int32)\n",
        "  rec[\"input_ids\"] = rec[\"input_ids\"][:max_seq_len]\n",
        "  rec[\"input_ids\"] = tf.pad(rec[\"input_ids\"], paddings=[[0, padding]])\n",
        "  \n",
        "  # token_type_ids\n",
        "  rec[\"token_type_ids\"] = tf.io.parse_tensor(rec[\"token_type_ids\"], out_type=tf.int32)\n",
        "  rec[\"token_type_ids\"] = rec[\"token_type_ids\"][:max_seq_len]\n",
        "  rec[\"token_type_ids\"] = tf.pad(rec[\"token_type_ids\"], paddings=[[0, padding]])\n",
        " \n",
        "  # shape bug?\n",
        "  # rec[\"text/embedding\"] = tf.reshape(rec[\"text/embedding\"], [64, 256])\n",
        "  rec[\"attention_mask\"] = tf.reshape(rec[\"attention_mask\"], [max_seq_len, ])\n",
        "  rec[\"input_ids\"] = tf.reshape(rec[\"input_ids\"], [max_seq_len, ])\n",
        "  rec[\"token_type_ids\"] = tf.reshape(rec[\"token_type_ids\"], [max_seq_len, ])\n",
        " \n",
        "  labels = tf.one_hot(rec[\"label\"], depth=4)\n",
        "  inputs = rec\n",
        "  inputs.pop(\"label\")\n",
        "\n",
        "  return inputs, labels\n",
        "\n",
        "train_raw = tf.data.TFRecordDataset(\"gs://tf_experiments_records/PolEmo/bert/train_chunk_0.TFRecord\", num_parallel_reads=4)\n",
        "example_proto = next(iter(train_raw))\n",
        "_parse_data(example_proto, max_seq_len=12)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'attention_mask': <tf.Tensor: shape=(12,), dtype=int32, numpy=array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0], dtype=int32)>,\n",
              "  'input_ids': <tf.Tensor: shape=(12,), dtype=int32, numpy=\n",
              "  array([  101,   348, 11335, 54276,   118, 10183, 10113, 74552,   119,\n",
              "           102,     0,     0], dtype=int32)>,\n",
              "  'token_type_ids': <tf.Tensor: shape=(12,), dtype=int32, numpy=array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>},\n",
              " <tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 1., 0., 0.], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMDV4evZB7OS",
        "colab_type": "code",
        "outputId": "b0790eb3-60dc-495a-8787-4d772a590a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "BATCH_SIZE = 8*64\n",
        "TRAINING_SIZE = 40000\n",
        "STEPS_PER_EPOCH = int(np.floor(TRAINING_SIZE / BATCH_SIZE)) + 1\n",
        "VALIDATION_STEPS = int(np.floor(5700 / BATCH_SIZE))\n",
        "\n",
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "\n",
        "print(f\"STEPS_PER_EPOCH: {STEPS_PER_EPOCH}\")\n",
        "print(f\"VALIDATION_STEPS: {VALIDATION_STEPS}\")\n",
        "\n",
        "filenames = [f\"gs://tf_experiments_records/PolEmo/bert/train_chunk_{chunk}.TFRecord\" for chunk in range(40)]\n",
        "\n",
        "train_raw = tf.data.TFRecordDataset(filenames, num_parallel_reads=1)\n",
        "train_parsed = train_raw.map(_parse_data).batch(BATCH_SIZE, drop_remainder=True).repeat(200)\n",
        "train_parsed = train_parsed.prefetch(-1)\n",
        "\n",
        "inputs, labels = next(iter(train_parsed))\n",
        "\n",
        "dev_raw = tf.data.TFRecordDataset(\"gs://tf_experiments_records/PolEmo/bert/dev.TFRecord\", num_parallel_reads=1)\n",
        "dev_parsed = dev_raw.map(_parse_data).batch(BATCH_SIZE, drop_remainder=True)\n",
        "dev_parsed = dev_parsed.prefetch(-1)\n",
        "\n",
        "test_raw = tf.data.TFRecordDataset(\"gs://tf_experiments_records/PolEmo/bert/test.TFRecord\", num_parallel_reads=1)\n",
        "test_parsed = test_raw.map(_parse_data).batch(BATCH_SIZE, drop_remainder=True)\n",
        "test_parsed = test_parsed.prefetch(-1)\n",
        "\n",
        "inputs"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.111.174.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system 10.111.174.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.111.174.138:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.111.174.138:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "STEPS_PER_EPOCH: 79\n",
            "VALIDATION_STEPS: 11\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': <tf.Tensor: shape=(512, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0],\n",
              "        [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>,\n",
              " 'input_ids': <tf.Tensor: shape=(512, 128), dtype=int32, numpy=\n",
              " array([[  101,   348, 11335, ...,     0,     0,     0],\n",
              "        [  101, 10685, 10563, ...,     0,     0,     0],\n",
              "        [  101, 14925, 21978, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [  101, 52742, 67935, ...,     0,     0,     0],\n",
              "        [  101, 82376, 12414, ...,     0,     0,     0],\n",
              "        [  101, 11255, 24378, ...,     0,     0,     0]], dtype=int32)>,\n",
              " 'token_type_ids': <tf.Tensor: shape=(512, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        ...,\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0],\n",
              "        [0, 0, 0, ..., 0, 0, 0]], dtype=int32)>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL97fVbGOND6",
        "colab_type": "text"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjTcYwZPOPsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  tf.random.set_seed(1234)\n",
        "  model = TFBertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', \n",
        "                                                          num_labels=4, \n",
        "                                                          hidden_dropout_prob=0.1, \n",
        "                                                          attention_probs_dropout_prob=0.1)\n",
        "\n",
        "  initial_learning_rate = 1e-5\n",
        "  lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "    initial_learning_rate,\n",
        "    decay_steps=STEPS_PER_EPOCH*10000,\n",
        "    decay_rate=.99,\n",
        "    staircase=True)\n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule), \n",
        "                loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
        "                metrics=[tf.keras.metrics.CategoricalAccuracy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3RwfR5sOiFL",
        "colab_type": "code",
        "outputId": "d912304d-0c7e-4342-e5ce-c982121fa9f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 464
        }
      },
      "source": [
        "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "model.fit(train_parsed, \n",
        "          validation_data=dev_parsed, \n",
        "          epochs=100,\n",
        "          steps_per_epoch=STEPS_PER_EPOCH, \n",
        "          validation_steps=VALIDATION_STEPS,\n",
        "          callbacks=[callback])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 79 steps, validate for 11 steps\n",
            "Epoch 1/100\n",
            "79/79 [==============================] - 115s 1s/step - loss: 1.0752 - categorical_accuracy: 0.5523 - val_loss: 0.9030 - val_categorical_accuracy: 0.6456\n",
            "Epoch 2/100\n",
            "79/79 [==============================] - 39s 490ms/step - loss: 0.8245 - categorical_accuracy: 0.6877 - val_loss: 0.8133 - val_categorical_accuracy: 0.6873\n",
            "Epoch 3/100\n",
            "79/79 [==============================] - 39s 491ms/step - loss: 0.7256 - categorical_accuracy: 0.7281 - val_loss: 0.7813 - val_categorical_accuracy: 0.7051\n",
            "Epoch 4/100\n",
            "79/79 [==============================] - 39s 493ms/step - loss: 0.6623 - categorical_accuracy: 0.7552 - val_loss: 0.7752 - val_categorical_accuracy: 0.7152\n",
            "Epoch 5/100\n",
            "79/79 [==============================] - 39s 493ms/step - loss: 0.6100 - categorical_accuracy: 0.7752 - val_loss: 0.7671 - val_categorical_accuracy: 0.7211\n",
            "Epoch 6/100\n",
            "79/79 [==============================] - 39s 492ms/step - loss: 0.5642 - categorical_accuracy: 0.7940 - val_loss: 0.7575 - val_categorical_accuracy: 0.7250\n",
            "Epoch 7/100\n",
            "79/79 [==============================] - 39s 492ms/step - loss: 0.5259 - categorical_accuracy: 0.8075 - val_loss: 0.7563 - val_categorical_accuracy: 0.7308\n",
            "Epoch 8/100\n",
            "79/79 [==============================] - 39s 490ms/step - loss: 0.4842 - categorical_accuracy: 0.8243 - val_loss: 0.7719 - val_categorical_accuracy: 0.7305\n",
            "Epoch 9/100\n",
            "79/79 [==============================] - 39s 491ms/step - loss: 0.4509 - categorical_accuracy: 0.8389 - val_loss: 0.7924 - val_categorical_accuracy: 0.7303\n",
            "Epoch 10/100\n",
            "79/79 [==============================] - 39s 491ms/step - loss: 0.4178 - categorical_accuracy: 0.8514 - val_loss: 0.8296 - val_categorical_accuracy: 0.7282\n",
            "Epoch 11/100\n",
            "79/79 [==============================] - 39s 492ms/step - loss: 0.3815 - categorical_accuracy: 0.8648 - val_loss: 0.8654 - val_categorical_accuracy: 0.7198\n",
            "Epoch 12/100\n",
            "79/79 [==============================] - 39s 493ms/step - loss: 0.3426 - categorical_accuracy: 0.8799 - val_loss: 0.8987 - val_categorical_accuracy: 0.7260\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f197bff8ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbNfbWU4zQIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1000: 58.20\n",
        "# 10000: 67.35\n",
        "# 20000: 70.33\n",
        "# 40000: 73.08"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}